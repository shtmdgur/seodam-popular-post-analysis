{"cells":[{"cell_type":"markdown","metadata":{"id":"IT1dEmdM2Kcv"},"source":["# Phase 2: Instruction Tuning ë°ì´í„°ì…‹ êµ¬ì¶•\n","\n","Phase 1ì—ì„œ ë¶„ì„í•œ ì¸ê¸°ê¸€ ë°ì´í„°ë¥¼ í™œìš©í•´ Instruction Tuningìš© JSONL ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"]},{"cell_type":"markdown","metadata":{"id":"XWunuoOk2Kcx"},"source":["## âœ… ëª©í‘œ\n","- ì¸ê¸°ê¸€ ì •ì œ ë°ì´í„°(`ì¸ê¸°ê¸€_clean.parquet`) ë¶ˆëŸ¬ì˜¤ê¸°\n","- ê²Œì‹œíŒëª… ì •ê·œí™” (`board_clean`)\n","- í‚¤ì›Œë“œ ì¶”ì¶œ(Mecab ê¸°ë°˜, ë¶ˆìš©ì–´ ì œê±°)\n","- Instruction/Input/Output í¬ë§·ìœ¼ë¡œ JSONL ìƒì„± (`outputs/training_dataset.jsonl`)\n","- ìƒ˜í”Œ ë ˆì½”ë“œ í”„ë¦°íŠ¸ë¡œ ê²€ì¦\n"]},{"cell_type":"markdown","metadata":{"id":"0oGg0slv2Kcy"},"source":["## âš™ï¸ ì‚¬ì „ ì¡°ê±´\n","- `phase0_setup.ipynb` ì‹¤í–‰ìœ¼ë¡œ `outputs/ì¸ê¸°ê¸€_clean.parquet` ì¡´ì¬\n","- (ì„ íƒ) `phase1_topic_modeling.ipynb` ì‹¤í–‰ìœ¼ë¡œ `ì¸ê¸°ê¸€_topics_for_prompt.json` ë“± ì¶”ê°€ ë©”íƒ€ í™œìš© ê°€ëŠ¥\n","- Colab ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹œ ì•„ë˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰\n"]},{"cell_type":"markdown","metadata":{"id":"lUxpDcJH2Kcz"},"source":["---\n","## 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•„ìš” ì‹œ)\n","- Mecab/konlpyê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ ë¨¼ì € ì„¤ì¹˜í•˜ì„¸ìš”.\n","- ë§Œì•½ ì„¤ì¹˜ì— ì‹¤íŒ¨í•˜ë”ë¼ë„, ì´ ë…¸íŠ¸ë¶ì€ ìë™ìœ¼ë¡œ Mecab í† í¬ë‚˜ì´ì €ë¥¼ ê±´ë„ˆë›°ê³  **ë¬¸ìì—´ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ë°©ì‹**ìœ¼ë¡œ fallback í•˜ë„ë¡ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWGtWuXD2Kcz"},"outputs":[],"source":["%pip install -q pandas numpy konlpy mecab-python3\n","\n","print(\"âœ… ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸\")\n"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Google Drive ë§ˆìš´íŠ¸ (ì´ë¯¸ ë§ˆìš´íŠ¸ë˜ì–´ ìˆë‹¤ë©´ ê±´ë„ˆë›°ì–´ë„ ë©ë‹ˆë‹¤)\n","drive.mount('/content/drive', force_remount=True)\n","\n","print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\")\n"],"metadata":{"id":"1HEtT2892LLC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xREskfGz2Kc0"},"source":["---\n","## 1. ê²½ë¡œ ì„¤ì • ë° ë°ì´í„° ë¡œë“œ\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6bXL0FB2Kc1"},"outputs":[],"source":["from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","from IPython.display import display\n","\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive/board_crawling\")\n","OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n","OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","HOT_PARQUET = OUTPUT_DIR / \"ì¸ê¸°ê¸€_clean.parquet\"\n","if not HOT_PARQUET.exists():\n","    raise FileNotFoundError(f\"{HOT_PARQUET}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Phase 0ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n","\n","hot_df = pd.read_parquet(HOT_PARQUET)\n","print(f\"âœ… ì¸ê¸°ê¸€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(hot_df):,}í–‰\")\n"]},{"cell_type":"markdown","metadata":{"id":"Ledthnwg2Kc1"},"source":["---\n","## 2. ê²Œì‹œíŒëª… ì •ê·œí™”\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"An4gbhLy2Kc2"},"outputs":[],"source":["BOARD_CANONICAL_MAP = {\n","    \"ìµê²Œ2\": \"ìµê²Œ2\",\n","    \"|ìµê²Œ2\": \"ìµê²Œ2\",\n","    \"ìµëª…2\": \"ìµê²Œ2\",\n","    \"ìµëª…ê²Œ2\": \"ìµê²Œ2\",\n","    \"ìµê²Œ1\": \"ìµê²Œ1\",\n","    \"|ìµê²Œ1\": \"ìµê²Œ1\",\n","    \"ì—°ì• \": \"ì—°ì• ìƒë‹´ì†Œ\",\n","    \"ì—°ì• ìƒë‹´ì†Œ\": \"ì—°ì• ìƒë‹´ì†Œ\",\n","    \"ììœ \": \"ììœ ê²Œì‹œíŒ\",\n","    \"ììœ ê²Œì‹œíŒ\": \"ììœ ê²Œì‹œíŒ\",\n","}\n","\n","BOARD_FALLBACK_RULES = {\n","    \"ik2\": \"ìµê²Œ2\",\n","    \"ik-2\": \"ìµê²Œ2\",\n","    \"ik1\": \"ìµê²Œ1\",\n","    \"love\": \"ì—°ì• ìƒë‹´ì†Œ\",\n","    \"free\": \"ììœ ê²Œì‹œíŒ\",\n","}\n","\n","\n","def normalize_board_name(raw_value: str, url_value: str) -> str:\n","    raw_value = (raw_value or \"\").strip().replace(\"|\", \"\").replace(\"#\", \"\")\n","    if raw_value in BOARD_CANONICAL_MAP:\n","        return BOARD_CANONICAL_MAP[raw_value]\n","\n","    lowered_raw = raw_value.lower()\n","    for key, target in BOARD_FALLBACK_RULES.items():\n","        if key in lowered_raw:\n","            return target\n","\n","    lowered_url = (url_value or \"\").lower()\n","    for key, target in BOARD_FALLBACK_RULES.items():\n","        if key in lowered_url:\n","            return target\n","\n","    return raw_value or \"ë¯¸ìƒ\"\n","\n","hot_df[\"board_clean\"] = hot_df.apply(\n","    lambda row: normalize_board_name(row.get(\"board\", \"\"), row.get(\"url\", \"\")),\n","    axis=1,\n",")\n","print(\"ğŸ“Š ê²Œì‹œíŒ ë¶„í¬:\")\n","print(hot_df[\"board_clean\"].value_counts().head(10))\n"]},{"cell_type":"markdown","metadata":{"id":"28O4hJKs2Kc2"},"source":["---\n","## 3. í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (Mecab + ë¶ˆìš©ì–´)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gEQaEIoQ2Kc2"},"outputs":[],"source":["try:\n","    from konlpy.tag import Mecab\n","    mecab = Mecab()\n","    MECAB_AVAILABLE = True\n","    print(\"âœ… Mecab ì‚¬ìš© ê°€ëŠ¥\")\n","except Exception as e:\n","    MECAB_AVAILABLE = False\n","    mecab = None\n","    print(f\"âš ï¸ Mecab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n","\n","import re\n","from collections import Counter\n","\n","DEFAULT_STOPWORDS = [\n","    \"ì´\",\"ê°€\",\"ì„\",\"ë¥¼\",\"ì—\",\"ì˜\",\"ì™€\",\"ê³¼\",\"ë„\",\"ë¡œ\",\"ìœ¼ë¡œ\",\n","    \"ì€\",\"ëŠ”\",\"ì—ì„œ\",\"ì—ê²Œ\",\"ê»˜\",\"í•œí…Œ\",\n","    \"ë”\",\"ê·¸\",\"ì €\",\"ì´ê²ƒ\",\"ê·¸ê²ƒ\",\"ì €ê²ƒ\",\"ê·¸ëŸ°\",\"ì´ëŸ°\",\"ì €ëŸ°\",\"ê·¸ë ‡\",\"ì´ë ‡\",\"ì €ë ‡\",\n","    \"ë•Œ\",\"ë•Œë¬¸\",\"ê²ƒ\",\"ìˆ˜\",\"ë“±\",\n","    \"ìˆë‹¤\",\"ì—†ë‹¤\",\"í•˜ë‹¤\",\"ë˜ë‹¤\",\"ì´ë‹¤\",\"ì•„ë‹ˆë‹¤\",\"ê°™ë‹¤\",\n","    \"ê²Œì‹œíŒ\",\"ê²Œì‹œê¸€\",\"ê¸€\",\"ëŒ“ê¸€\",\"ì‘ì„±\",\"ì‘ì„±ì\",\"ì¡°íšŒ\",\"ì¶”ì²œ\",\"ë¹„ì¶”ì²œ\"\n","]\n","CUSTOM_STOPWORDS = set(DEFAULT_STOPWORDS)\n","\n","\n","def extract_keywords_from_text(text: str, top_k: int = 8,\n","                               pos_filter=('NNG', 'NNP', 'VV', 'VA', 'MM'),\n","                               min_len: int = 2):\n","    if pd.isna(text) or not text:\n","        return []\n","\n","    tokens = []\n","    if MECAB_AVAILABLE and mecab is not None:\n","        try:\n","            tokens = [\n","                word for word, pos in mecab.pos(text)\n","                if (pos_filter is None or pos in pos_filter) and len(word) >= min_len\n","            ]\n","        except Exception as exc:\n","            print(f\"âš ï¸ Mecab í† í°í™” ì‹¤íŒ¨: {exc}\")\n","            tokens = re.findall(r\"[ê°€-í£a-zA-Z0-9]+\", text)\n","    else:\n","        tokens = re.findall(r\"[ê°€-í£a-zA-Z0-9]+\", text)\n","\n","    filtered = [tok for tok in tokens if tok not in CUSTOM_STOPWORDS]\n","    counts = Counter(filtered)\n","    return [token for token, _ in counts.most_common(top_k)]\n","\n","\n","def attach_keywords(df: pd.DataFrame, top_k: int = 8) -> pd.DataFrame:\n","    df = df.copy()\n","    combined = df[\"title\"].fillna(\"\") + \" \" + df[\"contents\"].fillna(\"\")\n","    df[\"keyword_list\"] = combined.apply(lambda text: extract_keywords_from_text(text, top_k=top_k))\n","    df[\"keywords\"] = df[\"keyword_list\"].apply(lambda ks: \", \".join(ks))\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlE9Osee2Kc3"},"outputs":[],"source":["hot_with_keywords = attach_keywords(hot_df, top_k=8)\n","print(\"ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¯¸ë¦¬ë³´ê¸°:\")\n","display(hot_with_keywords[[\"id\", \"board_clean\", \"keywords\"]].head(5))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPZyOu7y2Kc3"},"outputs":[],"source":["hot_with_keywords_path = OUTPUT_DIR / \"ì¸ê¸°ê¸€_with_keywords.parquet\"\n","hot_with_keywords.to_parquet(hot_with_keywords_path, index=False)\n","print(f\"ğŸ’¾ í‚¤ì›Œë“œ ë¶€ì°© ë°ì´í„° ì €ì¥: {hot_with_keywords_path}\")\n"]},{"cell_type":"markdown","metadata":{"id":"k-o35T0Q2Kc3"},"source":["---\n","## 4. Instruction Tuning JSONL ìƒì„±\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3BYrEMQ2Kc3"},"outputs":[],"source":["import json\n","from datetime import datetime\n","\n","def create_instruction_dataset(df: pd.DataFrame, output_path: Path, sample_size: int = None):\n","    records = []\n","    source_df = df if sample_size is None else df.sample(n=sample_size, random_state=42)\n","    for _, row in source_df.iterrows():\n","        keywords = row.get(\"keyword_list\", [])\n","        if not keywords:\n","            continue\n","\n","        board_name = row.get(\"board_clean\", \"ë¯¸ìƒ\")\n","        title = str(row.get(\"title\", \"\")).strip()\n","        contents = str(row.get(\"contents\", \"\")).strip()\n","        full_text = f\"{title}\\n\\n{contents}\".strip()\n","        if not full_text:\n","            continue\n","\n","        instruction = f\"ì£¼ì–´ì§„ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ {board_name}ì˜ ì¸ê¸°ê¸€ ìŠ¤íƒ€ì¼ë¡œ ê¸€ì„ ì‘ì„±í•´ì¤˜.\"\n","        input_text = f\"í‚¤ì›Œë“œ: {', '.join(keywords)}\"\n","\n","        records.append({\n","            \"instruction\": instruction,\n","            \"input\": input_text,\n","            \"output\": full_text,\n","        })\n","\n","    output_path.parent.mkdir(parents=True, exist_ok=True)\n","    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n","        for record in records:\n","            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n","\n","    print(f\"âœ… JSONL ìƒì„± ì™„ë£Œ: {len(records)}ê°œ ë ˆì½”ë“œ â†’ {output_path}\")\n","    return records\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LInGfUb22Kc4"},"outputs":[],"source":["jsonl_path = OUTPUT_DIR / \"training_dataset.jsonl\"\n","training_records = create_instruction_dataset(hot_with_keywords, jsonl_path)\n","\n","print(\"\\nìƒ˜í”Œ ë°ì´í„° (ìµœëŒ€ 3ê±´):\")\n","for i, rec in enumerate(training_records[:3], 1):\n","    print(f\"\\n[{i}]\")\n","    print(f\"Instruction: {rec['instruction']}\")\n","    print(f\"Input: {rec['input']}\")\n","    print(f\"Output (ì• 120ì): {rec['output'][:120]}...\")\n"]},{"cell_type":"markdown","metadata":{"id":"lMGpFcJX2Kc4"},"source":["---\n","## 5. ì™„ë£Œ ë° ë‹¤ìŒ ë‹¨ê³„\n","- ìƒì„±ëœ `training_dataset.jsonl`ì€ Phase 3 íŒŒì¸íŠœë‹ ë…¸íŠ¸ë¶ì—ì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","- í•„ìš” ì‹œ `hot_with_keywords` Parquetì„ ì°¸ê³ í•˜ì—¬ ì¶”ê°€ í•„í„°ë§/ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n","- ë‹¤ìŒ ë‹¨ê³„: `phase3_finetune.ipynb`ì—ì„œ QLoRA í•™ìŠµ ì§„í–‰\n"]},{"cell_type":"markdown","metadata":{"id":"VWScNAs02Kc4"},"source":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}