{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-9vwpAj2NwK"
   },
   "source": [
    "# Phase 3: Llama-3-8B QLoRA íŒŒì¸íŠœë‹\n",
    "\n",
    "Phase 2ì—ì„œ ìƒì„±í•œ `training_dataset.jsonl`ì„ ì‚¬ìš©í•˜ì—¬ Unsloth ê¸°ë°˜ QLoRA íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•˜ê³ , 16bit ë³‘í•© ëª¨ë¸ì„ Google Driveì— ì €ì¥í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ijk0erG42NwN"
   },
   "source": [
    "## âš™ï¸ ì‚¬ì „ ì¡°ê±´\n",
    "- `phase2_instruction_dataset.ipynb` ì‹¤í–‰ìœ¼ë¡œ `board_crawling/outputs/training_dataset.jsonl` ì¡´ì¬\n",
    "- Colab T4 ì´ìƒ GPU í• ë‹¹\n",
    "- Google Driveì— í”„ë¡œì íŠ¸ í´ë” ì—…ë¡œë“œ ì™„ë£Œ\n",
    "- ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹œ ì•„ë˜ ì„¤ì¹˜ ì…€ ì¬ì‹¤í–‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov1vr8PT2NwO"
   },
   "source": [
    "---\n",
    "## 0. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Unsloth + TRL + xformers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yvkxfZ9x2NwP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "âœ… Unsloth & ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "%pip install -q --no-deps \"trl<0.9.0\" \"peft<0.10.0\" \"accelerate<1.0.0\" \"bitsandbytes<0.43.0\"\n",
    "\n",
    "print(\"âœ… Unsloth & ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3m4k2iOa2Oad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¡œì»¬ í™˜ê²½\n"
     ]
    }
   ],
   "source": [
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸ (ì´ë¯¸ ë§ˆìš´íŠ¸ë˜ì–´ ìˆë‹¤ë©´ ê±´ë„ˆë›°ì–´ë„ ë©ë‹ˆë‹¤)\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAASZ7M52NwQ"
   },
   "source": [
    "---\n",
    "## 1. ê²½ë¡œ ë° ë°ì´í„° ë¡œë“œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ë…¸íŠ¸ë¶ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Colab í™˜ê²½ - Google Drive ê²½ë¡œ ì„¤ì •\n",
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/board_crawling\")\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_JSONL = OUTPUT_DIR / \"training_dataset.jsonl\"\n",
    "if not TRAIN_JSONL.exists():\n",
    "    raise FileNotFoundError(f\"{TRAIN_JSONL}ê°€ ì—†ìŠµë‹ˆë‹¤. Phase 2ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "\n",
    "training_data = []\n",
    "with open(TRAIN_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            training_data.append(json.loads(line))\n",
    "\n",
    "print(f\"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(training_data)}ê°œ ë ˆì½”ë“œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwMmhxF32NwQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: /content\n",
      "âš ï¸ Windows ê²½ë¡œ ì ‘ê·¼ ë¶ˆê°€: D:\\0.Sogang\\6\\ì›¹ë°ì´í„°ìˆ˜ì§‘ê³¼ í…ìŠ¤íŠ¸ë¶„ì„\\íŒ€í”Œ - ì„œë‹´ ì¸ê¸°ê¸€ ë¶„ì„ ë° ëª¨ë¸ë§\\board_crawling\\outputs\\training_dataset.jsonl\n",
      "ğŸ” training_dataset.jsonl íŒŒì¼ ê²€ìƒ‰ ì¤‘...\n",
      "âš ï¸ training_dataset.jsonlì„ ì°¾ì§€ ëª»í•´ ë…¸íŠ¸ë¶ íŒŒì¼ë¡œ ê²€ìƒ‰...\n",
      "âš ï¸ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©: board_crawling\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "training_dataset.jsonl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\ní˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: /content\ní”„ë¡œì íŠ¸ ë£¨íŠ¸: /content/board_crawling\nì˜ˆìƒ ê²½ë¡œ: /content/board_crawling/outputs/training_dataset.jsonl\n\nğŸ’¡ í•´ê²° ë°©ë²•:\n1. íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸: board_crawling/outputs/training_dataset.jsonl\n2. VSCodeì—ì„œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë¥¼ ë…¸íŠ¸ë¶ì´ ìˆëŠ” í´ë”ë¡œ ì—´ê¸°\n3. ë¡œì»¬ Python ì»¤ë„ ì‚¬ìš© ê³ ë ¤",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-448550784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;31m# íŒŒì¼ ì¡´ì¬ í™•ì¸\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTRAIN_JSONL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;34mf\"training_dataset.jsonl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;34mf\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {Path.cwd()}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: training_dataset.jsonl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\ní˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: /content\ní”„ë¡œì íŠ¸ ë£¨íŠ¸: /content/board_crawling\nì˜ˆìƒ ê²½ë¡œ: /content/board_crawling/outputs/training_dataset.jsonl\n\nğŸ’¡ í•´ê²° ë°©ë²•:\n1. íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸: board_crawling/outputs/training_dataset.jsonl\n2. VSCodeì—ì„œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë¥¼ ë…¸íŠ¸ë¶ì´ ìˆëŠ” í´ë”ë¡œ ì—´ê¸°\n3. ë¡œì»¬ Python ì»¤ë„ ì‚¬ìš© ê³ ë ¤"
     ]
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = Dataset.from_list(training_data)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "print(\"âœ… ë°ì´í„°ì…‹ í…œí”Œë¦¿ ì ìš© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlWCQyi92NwQ"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = Dataset.from_list(training_data)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "print(\"âœ… ë°ì´í„°ì…‹ í…œí”Œë¦¿ ì ìš© ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YU-NDxZo2NwR"
   },
   "source": [
    "---\n",
    "## 2. Unsloth ëª¨ë¸ ë¡œë“œ ë° QLoRA ì¤€ë¹„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5v8XMBq2NwR"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"âœ… Unsloth ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL9JzYbI2NwR"
   },
   "source": [
    "---\n",
    "## 3. SFTTrainer ì„¤ì • ë° í•™ìŠµ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oe_-Ajmt2NwR"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=str(OUTPUT_DIR / \"finetune_logs\"),\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ì„¤ì • ì™„ë£Œ. í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "trainer.train()\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1m8Wgt02NwS"
   },
   "source": [
    "---\n",
    "## 4. ëª¨ë¸ ì €ì¥ (LoRA + 16bit ë³‘í•©)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YRKNOyr2NwS"
   },
   "outputs": [],
   "source": [
    "OUTPUT_MODEL_DIR = MODEL_DIR / \"llama3_popular_post_lora\"\n",
    "OUTPUT_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# LoRA ì–´ëŒ‘í„° ì €ì¥\n",
    "model.save_pretrained(str(OUTPUT_MODEL_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_MODEL_DIR))\n",
    "print(f\"âœ… LoRA ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: {OUTPUT_MODEL_DIR}\")\n",
    "\n",
    "# 16bit ë³‘í•© ëª¨ë¸ ì €ì¥\n",
    "MERGED_DIR = OUTPUT_MODEL_DIR / \"merged_16bit\"\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.save_pretrained_merged(\n",
    "    MERGED_DIR,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "print(f\"âœ… 16bit ë³‘í•© ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MERGED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOfinG5J2NwS"
   },
   "source": [
    "> ğŸ’¡ **Tip**: í•„ìš” ì‹œ `MERGED_DIR`ë¥¼ `.zip`ìœ¼ë¡œ ì••ì¶•í•˜ì—¬ Google Drive/Hub ë“±ì— ì—…ë¡œë“œí•˜ë©´ Colab ì„¸ì…˜ì´ ì¢…ë£Œë˜ì–´ë„ ë³´ê´€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWIho1lF2NwS"
   },
   "source": [
    "---\n",
    "## 5. ë‹¤ìŒ ë‹¨ê³„\n",
    "- `phase4_generation.ipynb`ì—ì„œ ìœ„ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì¶”ë¡ /ì¸ê¸°ê¸€ ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "- ì¶”ê°€ë¡œ Hugging Face Hubì— ì—…ë¡œë“œí•˜ë ¤ë©´ `model.push_to_hub()`ë¥¼ í™œìš©í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnWnDEPn2NwT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAz1Iq2z2NwT"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
