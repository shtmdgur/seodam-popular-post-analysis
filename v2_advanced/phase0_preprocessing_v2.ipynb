{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeUjFr_e4d3t"
      },
      "source": [
        "# Phase 0v2: ë°ì´í„° ì „ì²˜ë¦¬ + ë…¸ì´ì¦ˆ ì œê±° ê°•í™”\n",
        "\n",
        "## ğŸ“Œ ëª©ì \n",
        "ê¸°ì¡´ Phase 0ì˜ ì „ì²˜ë¦¬(CSV ë¡œë“œ, ë‚ ì§œ í•„í„°ë§, ì¤‘ë³µ ì œê±°)ë¥¼ ìˆ˜í–‰í•œ ë’¤,\n",
        "**5ê°€ì§€ ë…¸ì´ì¦ˆ í•„í„°ë§ ê·œì¹™**ì„ ì ìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„° í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ“‚ ì¶œë ¥ íŒŒì¼\n",
        "```\n",
        "outputs/\n",
        "â”œâ”€â”€ ì¸ê¸°ê¸€_clean_v2.parquet          # ë…¸ì´ì¦ˆ ì œê±°ëœ ì¸ê¸°ê¸€\n",
        "â”œâ”€â”€ noise_removal_report.json        # í•„í„°ë§ í†µê³„ ë¦¬í¬íŠ¸\n",
        "â”œâ”€â”€ ìµê²Œ2_for_topic.parquet          # (ê¸°ì¡´ê³¼ ë™ì¼)\n",
        "â”œâ”€â”€ ììœ ê²Œì‹œíŒ_for_topic.parquet\n",
        "â”œâ”€â”€ ì—°ì• ìƒë‹´ì†Œ_for_topic.parquet\n",
        "â””â”€â”€ ìµê²Œ1_for_topic.parquet\n",
        "```\n",
        "\n",
        "## ğŸ”— ë‹¤ìŒ ë‹¨ê³„\n",
        "ì´ ë…¸íŠ¸ë¶ ì™„ë£Œ í›„ â†’ Phase 1(ê¸°ì¡´ í† í”½ ëª¨ë¸ë§) â†’ `phase2_dataset_v2.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3heSeSJM4d30"
      },
      "source": [
        "---\n",
        "## 1. Google Drive ë§ˆìš´íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRcqsgb54d31"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GubzeV5U4d32"
      },
      "source": [
        "---\n",
        "## 2. ê²½ë¡œ ì„¤ì • ë° íŒ¨í‚¤ì§€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wJ98Ycz4d33"
      },
      "outputs": [],
      "source": [
        "%pip install -q pandas numpy pyarrow\n",
        "\n",
        "print(\"âœ… ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBs2qes_4d33"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# â”€â”€ í”„ë¡œì íŠ¸ ê²½ë¡œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/board_crawling\")\n",
        "DATA_DIR     = PROJECT_ROOT / \"data\"\n",
        "OUTPUT_DIR   = PROJECT_ROOT / \"outputs\"\n",
        "MODEL_DIR    = PROJECT_ROOT / \"models\"\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ“ PROJECT_ROOT : {PROJECT_ROOT}\")\n",
        "print(f\"ğŸ“ DATA_DIR     : {DATA_DIR}\")\n",
        "print(f\"ğŸ“ OUTPUT_DIR   : {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL9xurdO4d34"
      },
      "source": [
        "---\n",
        "## 3. ìœ í‹¸ í•¨ìˆ˜ ì •ì˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd-LTkzL4d35"
      },
      "outputs": [],
      "source": [
        "def extract_id_from_url(u: str):\n",
        "    \"\"\"URLì—ì„œ 6ìë¦¬ ì´ìƒ ìˆ«ìë¥¼ ì¶”ì¶œí•˜ì—¬ post idë¡œ í™œìš©\"\"\"\n",
        "    if pd.isna(u):\n",
        "        return np.nan\n",
        "    m = re.search(r\"(\\d{6,})\", str(u))\n",
        "    return int(m.group(1)) if m else np.nan\n",
        "\n",
        "\n",
        "STD_COLS_BASE = [\n",
        "    \"id\", \"title\", \"url\", \"date\",\n",
        "    \"likes\", \"views\",\n",
        "    \"contents\", \"comments\", \"comments_num\",\n",
        "]\n",
        "\n",
        "\n",
        "def load_csv(path: Path, has_board: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"CSVë¥¼ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¡œ ë¡œë“œ + ê¸°ë³¸ ì •ì œ\"\"\"\n",
        "    print(f\"\\nğŸ“ Loading: {path.name}\")\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    cols = STD_COLS_BASE.copy()\n",
        "    if has_board:\n",
        "        cols.insert(4, \"board\")\n",
        "\n",
        "    for c in cols:\n",
        "        if c not in df.columns:\n",
        "            df[c] = np.nan\n",
        "\n",
        "    df = df[cols].copy()\n",
        "\n",
        "    df[\"title\"]    = df[\"title\"].astype(str).fillna(\"\")\n",
        "    df[\"url\"]      = df[\"url\"].astype(str).fillna(\"\")\n",
        "    df[\"contents\"] = df[\"contents\"].astype(str).fillna(\"\")\n",
        "    df[\"comments\"] = df[\"comments\"].astype(str).fillna(\"\")\n",
        "\n",
        "    if has_board:\n",
        "        df[\"board\"] = df[\"board\"].astype(str).fillna(\"\")\n",
        "\n",
        "    for c in [\"likes\", \"views\", \"comments_num\"]:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    df[\"id\"] = pd.to_numeric(df[\"id\"], errors=\"coerce\")\n",
        "    backup_id = df[\"url\"].apply(extract_id_from_url)\n",
        "    df[\"post_key\"] = df[\"id\"].fillna(backup_id).astype(\"Int64\")\n",
        "\n",
        "    dt = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
        "    dt = dt.dt.tz_convert(\"Asia/Seoul\").dt.tz_localize(None)\n",
        "    df[\"date\"] = dt\n",
        "\n",
        "    print(f\"  âœ“ {len(df):,}í–‰ ë¡œë“œ | date dtype: {df['date'].dtype}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "START_DT = pd.Timestamp(2024, 9, 1)\n",
        "END_DT   = pd.Timestamp(2025, 8, 31, 23, 59, 59)\n",
        "\n",
        "\n",
        "def filter_date_and_dedup(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
        "    before = len(df)\n",
        "    mask = (df[\"date\"] >= START_DT) & (df[\"date\"] <= END_DT)\n",
        "    df = df.loc[mask].copy()\n",
        "    after_date = len(df)\n",
        "\n",
        "    if \"post_key\" in df.columns:\n",
        "        df = df.sort_values(\"date\").drop_duplicates(subset=[\"post_key\"], keep=\"first\")\n",
        "    else:\n",
        "        df = df.sort_values(\"date\").drop_duplicates(subset=[\"url\"], keep=\"first\")\n",
        "\n",
        "    after_dedup = len(df)\n",
        "    print(f\"[{name}] {before:,} â†’ ë‚ ì§œ {after_date:,} â†’ ì¤‘ë³µì œê±° {after_dedup:,}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def ensure_post_key(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if \"post_key\" not in df.columns:\n",
        "        df[\"id\"] = pd.to_numeric(df[\"id\"], errors=\"coerce\")\n",
        "        backup = df[\"url\"].apply(extract_id_from_url)\n",
        "        df[\"post_key\"] = df[\"id\"].fillna(backup).astype(\"Int64\")\n",
        "    return df\n",
        "\n",
        "\n",
        "print(\"âœ… ìœ í‹¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy8EpEod4d36"
      },
      "source": [
        "---\n",
        "## 4. CSV ë°ì´í„° ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXSNQ3aD4d36"
      },
      "outputs": [],
      "source": [
        "# ì¼ë°˜ ê²Œì‹œíŒ ë¡œë“œ\n",
        "ik2_df  = load_csv(DATA_DIR / \"ìµê²Œ2_full_details.csv\")\n",
        "free_df = load_csv(DATA_DIR / \"ììœ ê²Œì‹œíŒ_full_details.csv\")\n",
        "love_df = load_csv(DATA_DIR / \"ì—°ì• ìƒë‹´ì†Œ_full_details.csv\")\n",
        "ik1_df  = load_csv(DATA_DIR / \"ìµê²Œ1_full_details.csv\")\n",
        "\n",
        "# ì¸ê¸°ê¸€ ë¡œë“œ (board í¬í•¨)\n",
        "hot_df  = load_csv(DATA_DIR / \"ì¸ê¸°ê¸€_full_details.csv\", has_board=True)\n",
        "\n",
        "print(\"\\nâœ… ëª¨ë“  CSV ë¡œë“œ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfI8DjQw4d36"
      },
      "source": [
        "---\n",
        "## 5. ë‚ ì§œ í•„í„°ë§ + ì¤‘ë³µ ì œê±° + ê²Œì‹œíŒ ì •ì œ (ê¸°ì¡´ Phase 0 ë™ì¼)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh0XwCm44d37"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸ“Š ë‚ ì§œ í•„í„°ë§ ë° ì¤‘ë³µ ì œê±° ì§„í–‰ ì¤‘...\\n\")\n",
        "\n",
        "ik2_df  = filter_date_and_dedup(ik2_df,  \"ìµê²Œ2\")\n",
        "free_df = filter_date_and_dedup(free_df, \"ììœ ê²Œì‹œíŒ\")\n",
        "love_df = filter_date_and_dedup(love_df, \"ì—°ì• ìƒë‹´ì†Œ\")\n",
        "ik1_df  = filter_date_and_dedup(ik1_df,  \"ìµê²Œ1\")\n",
        "hot_df  = filter_date_and_dedup(hot_df,  \"ì¸ê¸°ê¸€(raw)\")\n",
        "\n",
        "# ê²Œì‹œíŒëª… ì •ì œ\n",
        "def clean_board_name(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = str(s).strip().strip(\"|\")\n",
        "    s = re.sub(r\"[\\[\\]\\(\\)]\", \"\", s)\n",
        "    return s.strip()\n",
        "\n",
        "hot_df[\"board\"] = hot_df[\"board\"].astype(str).apply(clean_board_name)\n",
        "\n",
        "ALLOWED_BOARDS = [\"ììœ ê²Œì‹œíŒ\", \"ì—°ì• ìƒë‹´ì†Œ\", \"ìµê²Œ1\", \"ìµê²Œ2\"]\n",
        "hot_df = hot_df[hot_df[\"board\"].isin(ALLOWED_BOARDS)].copy()\n",
        "\n",
        "# post_key ë³´ê°•\n",
        "ik2_df  = ensure_post_key(ik2_df)\n",
        "free_df = ensure_post_key(free_df)\n",
        "love_df = ensure_post_key(love_df)\n",
        "ik1_df  = ensure_post_key(ik1_df)\n",
        "hot_df  = ensure_post_key(hot_df)\n",
        "\n",
        "print(f\"\\nğŸ“Š ì¸ê¸°ê¸€ ê²Œì‹œíŒ ë¶„í¬:\")\n",
        "print(hot_df[\"board\"].value_counts())\n",
        "print(f\"\\nâœ… ê¸°ë³¸ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(hot_df):,}ê±´\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPgxw35i4d37"
      },
      "source": [
        "---\n",
        "## 6. ğŸ†• ë…¸ì´ì¦ˆ ê²Œì‹œê¸€ ì œê±°\n",
        "\n",
        "í•™ìŠµ ë°ì´í„° í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ 5ê°€ì§€ ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "| í•„í„° | ê¸°ì¤€ | ê·¼ê±° |\n",
        "|------|------|------|\n",
        "| ë¹ˆ ì½˜í…ì¸  | contentsê°€ NaN/ë¹ˆë¬¸ìì—´/ê³µë°±ë§Œ | ë³¸ë¬¸ ì—†ëŠ” ê¸€ì€ í•™ìŠµ ë¶ˆê°€ |\n",
        "| ì´ˆë‹¨ë¬¸ | ë³¸ë¬¸ ê¸¸ì´ < 10ì | ë‹¨ìˆœ ë°˜ì‘ ì œê±° |\n",
        "| nan ë¬¸ìì—´ | title/contentsì— 'nan' í¬í•¨ | í¬ë¡¤ë§ ì˜¤ë¥˜ |\n",
        "| ê³¼ë„í•œ URL | URLì´ 3ê°œ ì´ìƒ | ê´‘ê³ /ìŠ¤íŒ¸ |\n",
        "| ì¤‘ë³µ ì œëª© | ë™ì¼ ì œëª© 3íšŒ ì´ìƒ | ë´‡/ë„ë°° |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igojclEx4d37"
      },
      "outputs": [],
      "source": [
        "def remove_noise_posts(df: pd.DataFrame) -> tuple:\n",
        "    \"\"\"\n",
        "    5ê°€ì§€ ê·œì¹™ ê¸°ë°˜ ë…¸ì´ì¦ˆ í•„í„°ë§.\n",
        "    Returns:\n",
        "        (ì •ì œëœ DataFrame, ì œê±° í†µê³„ dict, ì œê±° ë§ˆìŠ¤í¬)\n",
        "    \"\"\"\n",
        "    report = {\"before\": len(df), \"filters\": {}}\n",
        "    mask_keep = pd.Series(True, index=df.index)\n",
        "\n",
        "    # â”€â”€ í•„í„° 1: ë¹ˆ ì½˜í…ì¸  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    contents_clean = df[\"contents\"].fillna(\"\").str.strip()\n",
        "    f1 = (contents_clean == \"\") | (contents_clean == \"nan\")\n",
        "    report[\"filters\"][\"ë¹ˆ_ì½˜í…ì¸ \"] = int(f1.sum())\n",
        "    mask_keep &= ~f1\n",
        "\n",
        "    # â”€â”€ í•„í„° 2: ì´ˆë‹¨ë¬¸ (10ì ë¯¸ë§Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    f2 = contents_clean.str.len() < 10\n",
        "    # í•„í„° 1ì—ì„œ ì´ë¯¸ ì¡íŒ ê²ƒì€ ì¤‘ë³µ ì¹´ìš´íŠ¸ ë°©ì§€\n",
        "    f2_new = f2 & ~f1\n",
        "    report[\"filters\"][\"ì´ˆë‹¨ë¬¸_10ìë¯¸ë§Œ\"] = int(f2_new.sum())\n",
        "    mask_keep &= ~f2\n",
        "\n",
        "    # â”€â”€ í•„í„° 3: nan ë¬¸ìì—´ í¬í•¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    title_lower = df[\"title\"].fillna(\"\").str.strip().str.lower()\n",
        "    contents_lower = contents_clean.str.lower()\n",
        "    # titleì´ ì •í™•íˆ 'nan'ì´ê±°ë‚˜, contentsê°€ ì •í™•íˆ 'nan'ì¸ ê²½ìš°\n",
        "    f3 = (title_lower == \"nan\") | (contents_lower == \"nan\")\n",
        "    report[\"filters\"][\"nan_ë¬¸ìì—´\"] = int((f3 & ~f1 & ~f2).sum())\n",
        "    mask_keep &= ~f3\n",
        "\n",
        "    # â”€â”€ í•„í„° 4: ê³¼ë„í•œ URL (3ê°œ ì´ìƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    url_pattern = re.compile(r'https?://\\S+')\n",
        "    url_counts = contents_clean.apply(lambda x: len(url_pattern.findall(x)))\n",
        "    f4 = url_counts >= 3\n",
        "    f4_new = f4 & mask_keep\n",
        "    report[\"filters\"][\"ê³¼ë„í•œ_URL\"] = int(f4_new.sum())\n",
        "    mask_keep &= ~f4\n",
        "\n",
        "    # â”€â”€ í•„í„° 5: ì¤‘ë³µ ì œëª© (ë™ì¼ ì œëª© 3íšŒ ì´ìƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    title_clean = df[\"title\"].fillna(\"\").str.strip()\n",
        "    title_counts = title_clean.map(title_clean.value_counts())\n",
        "    f5 = (title_counts >= 3) & (title_clean != \"\") & (title_clean.str.lower() != \"nan\")\n",
        "    f5_new = f5 & mask_keep\n",
        "    report[\"filters\"][\"ì¤‘ë³µ_ì œëª©\"] = int(f5_new.sum())\n",
        "    mask_keep &= ~f5\n",
        "\n",
        "    # â”€â”€ ê²°ê³¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    removed_mask = ~mask_keep\n",
        "    df_clean = df[mask_keep].copy().reset_index(drop=True)\n",
        "    report[\"after\"] = len(df_clean)\n",
        "    report[\"total_removed\"] = report[\"before\"] - report[\"after\"]\n",
        "\n",
        "    return df_clean, report, removed_mask\n",
        "\n",
        "\n",
        "print(\"âœ… ë…¸ì´ì¦ˆ ì œê±° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND5hTi_94d38"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸ§¹ ë…¸ì´ì¦ˆ ê²Œì‹œê¸€ ì œê±° ì‹œì‘...\\n\")\n",
        "\n",
        "hot_clean_v2, noise_report, removed_mask = remove_noise_posts(hot_df)\n",
        "\n",
        "# â”€â”€ ë¦¬í¬íŠ¸ ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"=\" * 60)\n",
        "print(\"ë…¸ì´ì¦ˆ ì œê±° ë¦¬í¬íŠ¸\".center(60))\n",
        "print(\"=\" * 60)\n",
        "print(f\"  ì „ì²˜ë¦¬ ì „: {noise_report['before']:,}ê±´\")\n",
        "print(f\"  ì „ì²˜ë¦¬ í›„: {noise_report['after']:,}ê±´\")\n",
        "print(f\"  ì´ ì œê±°:   {noise_report['total_removed']:,}ê±´\")\n",
        "print(\"â”€\" * 60)\n",
        "print(\"  í•„í„°ë³„ ì œê±° ë¶„í¬:\")\n",
        "for filter_name, count in noise_report[\"filters\"].items():\n",
        "    print(f\"    {filter_name}: {count:,}ê±´\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# â”€â”€ ì œê±°ëœ ê²Œì‹œê¸€ ìƒ˜í”Œ í™•ì¸ (ì›ë˜ hot_dfì—ì„œ ì¶”ì¶œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "removed_sample = hot_df[removed_mask][[\"title\", \"contents\", \"board\"]].head(5)\n",
        "print(\"\\nğŸ” ì œê±°ëœ ê²Œì‹œê¸€ ìƒ˜í”Œ:\")\n",
        "for i, (_, row) in enumerate(removed_sample.iterrows(), 1):\n",
        "    title = str(row[\"title\"])[:50]\n",
        "    contents = str(row[\"contents\"])[:80]\n",
        "    print(f\"  [{i}] [{row['board']}] {title}\")\n",
        "    print(f\"      ë‚´ìš©: {contents}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9fw03Kq4d38"
      },
      "source": [
        "---\n",
        "## 7. ì¸ê¸°ê¸€ ì›ê¸€ ì œê±° + ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwD2Spke4d38"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ ì¸ê¸°ê¸€ ì›ê¸€ ì œê±° (í† í”½ ëª¨ë¸ë§ìš© ê²Œì‹œíŒ ë°ì´í„°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "hot_keys = set(hot_clean_v2[\"post_key\"].dropna().astype(\"Int64\"))\n",
        "print(f\"ğŸ“Œ ì¸ê¸°ê¸€ ê³ ìœ  post_key ìˆ˜: {len(hot_keys):,}\")\n",
        "\n",
        "\n",
        "def mask_hot(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
        "    before = len(df)\n",
        "    out = df[~df[\"post_key\"].isin(hot_keys)].copy()\n",
        "    removed = before - len(out)\n",
        "    print(f\"[{name}] {before:,} â†’ {len(out):,} (ì¸ê¸°ê¸€ ì œê±° {removed:,}ê°œ)\")\n",
        "    return out\n",
        "\n",
        "\n",
        "ik2_for_topic  = mask_hot(ik2_df,  \"ìµê²Œ2\")\n",
        "free_for_topic = mask_hot(free_df, \"ììœ ê²Œì‹œíŒ\")\n",
        "love_for_topic = mask_hot(love_df, \"ì—°ì• ìƒë‹´ì†Œ\")\n",
        "ik1_for_topic  = mask_hot(ik1_df,  \"ìµê²Œ1\")\n",
        "\n",
        "print(\"\\nâœ… ì¸ê¸°ê¸€ ì›ê¸€ ì œê±° ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HDt7JPJ4d39"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"ğŸ’¾ ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ì¤‘...\")\n",
        "\n",
        "# í† í”½ ëª¨ë¸ë§ìš© (ê¸°ì¡´ê³¼ ë™ì¼ ê²½ë¡œ)\n",
        "ik2_for_topic.to_parquet(OUTPUT_DIR / \"ìµê²Œ2_for_topic.parquet\", index=False)\n",
        "free_for_topic.to_parquet(OUTPUT_DIR / \"ììœ ê²Œì‹œíŒ_for_topic.parquet\", index=False)\n",
        "love_for_topic.to_parquet(OUTPUT_DIR / \"ì—°ì• ìƒë‹´ì†Œ_for_topic.parquet\", index=False)\n",
        "ik1_for_topic.to_parquet(OUTPUT_DIR / \"ìµê²Œ1_for_topic.parquet\", index=False)\n",
        "\n",
        "# ğŸ†• ë…¸ì´ì¦ˆ ì œê±°ëœ ì¸ê¸°ê¸€ (v2)\n",
        "hot_clean_v2.to_parquet(OUTPUT_DIR / \"ì¸ê¸°ê¸€_clean_v2.parquet\", index=False)\n",
        "\n",
        "# ğŸ†• í•„í„°ë§ ë¦¬í¬íŠ¸ ì €ì¥\n",
        "with open(OUTPUT_DIR / \"noise_removal_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(noise_report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"  âœ“ ìµê²Œ2_for_topic.parquet\")\n",
        "print(\"  âœ“ ììœ ê²Œì‹œíŒ_for_topic.parquet\")\n",
        "print(\"  âœ“ ì—°ì• ìƒë‹´ì†Œ_for_topic.parquet\")\n",
        "print(\"  âœ“ ìµê²Œ1_for_topic.parquet\")\n",
        "print(\"  âœ“ ì¸ê¸°ê¸€_clean_v2.parquet (ğŸ†• ë…¸ì´ì¦ˆ ì œê±°)\")\n",
        "print(\"  âœ“ noise_removal_report.json (ğŸ†• í•„í„°ë§ ë¦¬í¬íŠ¸)\")\n",
        "print(f\"\\nâœ… ëª¨ë“  íŒŒì¼ ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk_YRE8q4d39"
      },
      "source": [
        "---\n",
        "## 8. ì „ì²˜ë¦¬ ìš”ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UQB1lk04d3-"
      },
      "outputs": [],
      "source": [
        "summary_rows = [\n",
        "    {\"ê²Œì‹œíŒ\": \"ìµê²Œ2\",     \"ì „ì²˜ë¦¬ í›„\": f\"{len(ik2_for_topic):,}\"},\n",
        "    {\"ê²Œì‹œíŒ\": \"ììœ ê²Œì‹œíŒ\", \"ì „ì²˜ë¦¬ í›„\": f\"{len(free_for_topic):,}\"},\n",
        "    {\"ê²Œì‹œíŒ\": \"ì—°ì• ìƒë‹´ì†Œ\", \"ì „ì²˜ë¦¬ í›„\": f\"{len(love_for_topic):,}\"},\n",
        "    {\"ê²Œì‹œíŒ\": \"ìµê²Œ1\",     \"ì „ì²˜ë¦¬ í›„\": f\"{len(ik1_for_topic):,}\"},\n",
        "    {\"ê²Œì‹œíŒ\": \"ì¸ê¸°ê¸€(v2)\", \"ì „ì²˜ë¦¬ í›„\": f\"{len(hot_clean_v2):,}\"},\n",
        "]\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Phase 0v2 ì™„ë£Œ - ì „ì²˜ë¦¬ + ë…¸ì´ì¦ˆ ì œê±° ìš”ì•½\".center(60))\n",
        "print(\"=\" * 60)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nğŸ§¹ ë…¸ì´ì¦ˆ ì œê±°: {noise_report['before']:,} â†’ {noise_report['after']:,} ({noise_report['total_removed']:,}ê±´ ì œê±°)\")\n",
        "print(\"\\nğŸ‰ Phase 0v2 ì™„ë£Œ!\")\n",
        "print(\"ë‹¤ìŒ ë‹¨ê³„: Phase 1(ê¸°ì¡´ í† í”½ ëª¨ë¸ë§) â†’ phase2_dataset_v2.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "G4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}