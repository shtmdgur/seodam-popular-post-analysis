{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 3v2: QLoRA íŒŒì¸íŠœë‹ (SOS/EOS + CoT í•™ìŠµ)\n",
                "\n",
                "## ğŸ“Œ ëª©ì \n",
                "Phase 2v2ì—ì„œ ìƒì„±í•œ `training_dataset_v2.jsonl`ì„ ì‚¬ìš©í•˜ì—¬\n",
                "Unsloth ê¸°ë°˜ QLoRA íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
                "\n",
                "### ê¸°ì¡´ ëŒ€ë¹„ ê°œì„ ì \n",
                "- âœ… SOS/EOS í† í° í¬í•¨ â†’ ìƒì„± ì¢…ë£Œ ì‹œì  í•™ìŠµ\n",
                "- âœ… CoT(Thinking) í¬í•¨ â†’ ì¶”ë¡  ê³¼ì • í•™ìŠµ\n",
                "- âœ… ë…¸ì´ì¦ˆ ì œê±°ëœ ê³ í’ˆì§ˆ ë°ì´í„°\n",
                "- âœ… RPM ê¸°ë°˜ í”¼ì²˜/íŒ©í„° ì •ë³´ í¬í•¨\n",
                "\n",
                "## âš™ï¸ ì‚¬ì „ ì¡°ê±´\n",
                "- `phase2_dataset_v2.ipynb` ì‹¤í–‰ìœ¼ë¡œ `outputs/training_dataset_v2.jsonl` ì¡´ì¬\n",
                "- Colab T4 ì´ìƒ GPU í• ë‹¹\n",
                "\n",
                "## ğŸ“‚ ì¶œë ¥ íŒŒì¼\n",
                "```\n",
                "models/\n",
                "â”œâ”€â”€ llama3_popular_post_lora_v2/          # LoRA ì–´ëŒ‘í„°\n",
                "â””â”€â”€ llama3_popular_post_lora_v2/merged_16bit/  # 16bit ë³‘í•© ëª¨ë¸\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 0. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "%pip install -q --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" \"peft<0.10.0\" \"accelerate<1.0.0\" \"bitsandbytes<0.43.0\"\n",
                "%pip install -q datasets\n",
                "\n",
                "print(\"âœ… Unsloth & ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "\n",
                "drive.mount('/content/drive', force_remount=True)\n",
                "\n",
                "print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. ê²½ë¡œ ë° ë°ì´í„° ë¡œë“œ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import json\n",
                "from datasets import Dataset\n",
                "\n",
                "PROJECT_ROOT = Path(\"/content/drive/MyDrive/board_crawling\")\n",
                "OUTPUT_DIR   = PROJECT_ROOT / \"outputs\"\n",
                "MODEL_DIR    = PROJECT_ROOT / \"models\"\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "TRAIN_JSONL = OUTPUT_DIR / \"training_dataset_v2.jsonl\"\n",
                "if not TRAIN_JSONL.exists():\n",
                "    raise FileNotFoundError(f\"{TRAIN_JSONL}ê°€ ì—†ìŠµë‹ˆë‹¤. Phase 2v2ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
                "\n",
                "training_data = []\n",
                "with open(TRAIN_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
                "    for line in f:\n",
                "        if line.strip():\n",
                "            training_data.append(json.loads(line))\n",
                "\n",
                "print(f\"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(training_data):,}ê°œ ë ˆì½”ë“œ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Unsloth ëª¨ë¸ ë¡œë“œ + QLoRA ì„¤ì •"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "max_seq_length = 2048\n",
                "dtype = None\n",
                "load_in_4bit = True\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
                "    max_seq_length=max_seq_length,\n",
                "    dtype=dtype,\n",
                "    load_in_4bit=load_in_4bit,\n",
                ")\n",
                "\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=16,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha=32,\n",
                "    lora_dropout=0,\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",\n",
                "    random_state=3407,\n",
                "    use_rslora=False,\n",
                "    loftq_config=None,\n",
                ")\n",
                "\n",
                "print(f\"âœ… Unsloth ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
                "print(f\"  BOS token: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})\")\n",
                "print(f\"  EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. ë°ì´í„°ì…‹ í…œí”Œë¦¿ ì ìš© (SOS/EOS í† í° í™•ì¸)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ text í•„ë“œì˜ SOS/EOSë¥¼ ì‹¤ì œ í† í¬ë‚˜ì´ì € í† í°ìœ¼ë¡œ ëŒ€ì²´ â”€â”€â”€â”€â”€â”€\n",
                "BOS = tokenizer.bos_token  # <|begin_of_text|>\n",
                "EOS = tokenizer.eos_token  # <|end_of_text|>\n",
                "\n",
                "def fix_special_tokens(text: str) -> str:\n",
                "    \"\"\"placeholderë¥¼ ì‹¤ì œ í† í¬ë‚˜ì´ì € í† í°ìœ¼ë¡œ ëŒ€ì²´.\n",
                "    Llama-3ì—ì„œëŠ” placeholderì™€ ì‹¤ì œ í† í°ì´ ë™ì¼í•˜ë¯€ë¡œ\n",
                "    ì´ í•¨ìˆ˜ëŠ” ì•ˆì „ì¥ì¹˜ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
                "    ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ì‹œ ì—¬ê¸°ì„œ ë§¤í•‘ì„ ë³€ê²½í•˜ì„¸ìš”.\"\"\"\n",
                "    text = text.replace(\"<|begin_of_text|>\", BOS)\n",
                "    text = text.replace(\"<|end_of_text|>\", EOS)\n",
                "    return text\n",
                "\n",
                "for record in training_data:\n",
                "    if \"text\" in record:\n",
                "        record[\"text\"] = fix_special_tokens(record[\"text\"])\n",
                "\n",
                "dataset = Dataset.from_list(training_data)\n",
                "\n",
                "# ê²€ì¦: ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸\n",
                "print(\"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n",
                "print(f\"   ë ˆì½”ë“œ ìˆ˜: {len(dataset):,}\")\n",
                "print(f\"\\nğŸ“ ì²« ë²ˆì§¸ ìƒ˜í”Œ (ì• 300ì):\")\n",
                "print(dataset[0][\"text\"][:300])\n",
                "print(f\"\\nğŸ“ ë§ˆì§€ë§‰ 50ì:\")\n",
                "print(dataset[0][\"text\"][-50:])\n",
                "\n",
                "# í† í°í™” ê²€ì¦: BOS/EOSê°€ ì˜¬ë°”ë¥´ê²Œ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸\n",
                "sample_ids = tokenizer(dataset[0][\"text\"], return_tensors=\"pt\").input_ids[0]\n",
                "print(f\"\\nğŸ” í† í°í™” ê²€ì¦:\")\n",
                "print(f\"  ì²« í† í° ID: {sample_ids[0].item()} (BOS={tokenizer.bos_token_id})\")\n",
                "print(f\"  ë í† í° ID: {sample_ids[-1].item()} (EOS={tokenizer.eos_token_id})\")\n",
                "print(f\"  BOS í¬í•¨: {sample_ids[0].item() == tokenizer.bos_token_id}\")\n",
                "print(f\"  EOS í¬í•¨: {sample_ids[-1].item() == tokenizer.eos_token_id}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. SFTTrainer ì„¤ì • ë° í•™ìŠµ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "# â”€â”€ íŒ¨ë”© ì„¤ì • (EOS í•™ìŠµ ë³´ì¥) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=max_seq_length,\n",
                "    packing=False,\n",
                "    args=TrainingArguments(\n",
                "        per_device_train_batch_size=2,\n",
                "        gradient_accumulation_steps=4,\n",
                "        warmup_steps=5,\n",
                "        num_train_epochs=3,\n",
                "        learning_rate=2e-4,\n",
                "        fp16=not torch.cuda.is_bf16_supported(),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "        logging_steps=1,\n",
                "        optim=\"adamw_8bit\",\n",
                "        weight_decay=0.01,\n",
                "        lr_scheduler_type=\"linear\",\n",
                "        seed=3407,\n",
                "        output_dir=str(OUTPUT_DIR / \"finetune_logs_v2\"),\n",
                "        save_strategy=\"epoch\",\n",
                "        save_total_limit=3,\n",
                "    ),\n",
                ")\n",
                "\n",
                "print(\"âœ… Trainer ì„¤ì • ì™„ë£Œ. í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
                "trainer.train()\n",
                "print(\"âœ… í•™ìŠµ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. ëª¨ë¸ ì €ì¥ (LoRA + 16bit ë³‘í•©)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "OUTPUT_MODEL_DIR = MODEL_DIR / \"llama3_popular_post_lora_v2\"\n",
                "OUTPUT_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# LoRA ì–´ëŒ‘í„° ì €ì¥\n",
                "model.save_pretrained(str(OUTPUT_MODEL_DIR))\n",
                "tokenizer.save_pretrained(str(OUTPUT_MODEL_DIR))\n",
                "print(f\"âœ… LoRA ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: {OUTPUT_MODEL_DIR}\")\n",
                "\n",
                "# 16bit ë³‘í•© ëª¨ë¸ ì €ì¥\n",
                "MERGED_DIR = OUTPUT_MODEL_DIR / \"merged_16bit\"\n",
                "FastLanguageModel.for_inference(model)\n",
                "model.save_pretrained_merged(\n",
                "    str(MERGED_DIR),\n",
                "    tokenizer,\n",
                "    save_method=\"merged_16bit\",\n",
                ")\n",
                "print(f\"âœ… 16bit ë³‘í•© ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MERGED_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. ë‹¤ìŒ ë‹¨ê³„\n",
                "â†’ `phase4_generation_v2.ipynb`ì—ì„œ CoT ê¸°ë°˜ ì¸ê¸°ê¸€ ìƒì„± ìˆ˜í–‰"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "accelerator": "GPU",
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}