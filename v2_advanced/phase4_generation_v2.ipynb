{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 4v2: CoT ê¸°ë°˜ ì¸ê¸°ê¸€ ìƒì„±\n",
                "\n",
                "## ğŸ“Œ ëª©ì \n",
                "Phase 3v2ì—ì„œ ì €ì¥í•œ íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬,\n",
                "RPM íŒ©í„° ì •ë³´ì™€ CoT(Thinking â†’ Response) êµ¬ì¡°ë¥¼ í™œìš©í•œ ì¸ê¸°ê¸€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
                "\n",
                "### ê¸°ì¡´ ëŒ€ë¹„ ê°œì„ ì \n",
                "- âœ… íŒ©í„°(ì¸ê¸°ìœ í˜•) ì •ë³´ë¥¼ ì…ë ¥ìœ¼ë¡œ ì œê³µ â†’ ì˜ë„í•œ ìŠ¤íƒ€ì¼ ìƒì„±\n",
                "- âœ… CoT: ëª¨ë¸ì´ ë¨¼ì € ì „ëµì„ ì‚¬ê³ (Thinking) â†’ ê·¸ì— ë§ê²Œ ê¸€ ìƒì„±(Response)\n",
                "- âœ… EOS í† í° í•™ìŠµ ì™„ë£Œ â†’ ìì—°ìŠ¤ëŸ¬ìš´ ìƒì„± ì¢…ë£Œ\n",
                "\n",
                "## âš™ï¸ ì‚¬ì „ ì¡°ê±´\n",
                "- `phase2_dataset_v2.ipynb` ì‹¤í–‰ìœ¼ë¡œ `outputs/rpm_factors.json` ì¡´ì¬\n",
                "- `phase3_finetune_v2.ipynb` ì‹¤í–‰ìœ¼ë¡œ `models/llama3_popular_post_lora_v2` ì¡´ì¬\n",
                "- GPU ëŸ°íƒ€ì„ í• ë‹¹"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" pandas\n",
                "\n",
                "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "\n",
                "drive.mount('/content/drive', force_remount=True)\n",
                "\n",
                "print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. ê²½ë¡œ ì„¤ì • ë° íŒ©í„° ì •ë³´ ë¡œë“œ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import json\n",
                "import pandas as pd\n",
                "from IPython.display import display\n",
                "\n",
                "PROJECT_ROOT = Path(\"/content/drive/MyDrive/board_crawling\")\n",
                "OUTPUT_DIR   = PROJECT_ROOT / \"outputs\"\n",
                "MODEL_DIR    = PROJECT_ROOT / \"models\"\n",
                "\n",
                "# â”€â”€ RPM íŒ©í„° ì •ë³´ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "FACTORS_PATH = OUTPUT_DIR / \"rpm_factors.json\"\n",
                "if FACTORS_PATH.exists():\n",
                "    with open(FACTORS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
                "        factors_data = json.load(f)\n",
                "    factor_names = [f[\"factor_name\"] for f in factors_data[\"factors\"]]\n",
                "    factor_stats = factors_data.get(\"factor_stats\", {})\n",
                "    print(f\"âœ… íŒ©í„° ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(factor_names)}ê°œ íŒ©í„°\")\n",
                "    for name in factor_names:\n",
                "        stats = factor_stats.get(name, {})\n",
                "        coverage = stats.get(\"coverage\", 0)\n",
                "        print(f\"  â†’ {name} (ì»¤ë²„ë¦¬ì§€: {coverage:.0%})\")\n",
                "else:\n",
                "    print(\"âš ï¸ íŒ©í„° ì •ë³´ ì—†ìŒ, ê¸°ë³¸ ëª¨ë“œë¡œ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
                "    factor_names = [\"ì¼ë°˜\"]\n",
                "    factor_stats = {}\n",
                "\n",
                "# â”€â”€ í† í”½ í‚¤ì›Œë“œ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "PROMPT_FILES = {\n",
                "    \"ìµê²Œ2\":     OUTPUT_DIR / \"ìµê²Œ2_topics_for_prompt.json\",\n",
                "    \"ììœ ê²Œì‹œíŒ\": OUTPUT_DIR / \"ììœ ê²Œì‹œíŒ_topics_for_prompt.json\",\n",
                "    \"ì—°ì• ìƒë‹´ì†Œ\": OUTPUT_DIR / \"ì—°ì• ìƒë‹´ì†Œ_topics_for_prompt.json\",\n",
                "    \"ìµê²Œ1\":     OUTPUT_DIR / \"ìµê²Œ1_topics_for_prompt.json\",\n",
                "}\n",
                "\n",
                "trend_keywords = {}\n",
                "for board, path in PROMPT_FILES.items():\n",
                "    if not path.exists():\n",
                "        print(f\"âš ï¸ {board}: {path} ì—†ìŒ â†’ ê±´ë„ˆëœ€\")\n",
                "        continue\n",
                "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
                "        topics_data = json.load(f)\n",
                "    top_topics = sorted(topics_data, key=lambda x: len(x.get(\"representatives\", [])), reverse=True)[:5]\n",
                "    keywords = []\n",
                "    for topic in top_topics:\n",
                "        kw = topic.get(\"keywords\", [])\n",
                "        if isinstance(kw, list):\n",
                "            keywords.extend(kw[:8])\n",
                "    trend_keywords[board] = list(dict.fromkeys(keywords))[:8]\n",
                "    print(f\"âœ… {board}: {len(trend_keywords[board])}ê°œ í‚¤ì›Œë“œ\")\n",
                "\n",
                "if not trend_keywords:\n",
                "    raise RuntimeError(\"í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. Phase 1 ë…¸íŠ¸ë¶ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "FINETUNED_DIR = MODEL_DIR / \"llama3_popular_post_lora_v2\"\n",
                "if not FINETUNED_DIR.exists():\n",
                "    raise FileNotFoundError(f\"{FINETUNED_DIR}ê°€ ì—†ìŠµë‹ˆë‹¤. Phase 3v2ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=str(FINETUNED_DIR),\n",
                "    max_seq_length=2048,\n",
                "    dtype=None,\n",
                "    load_in_4bit=True,\n",
                ")\n",
                "\n",
                "FastLanguageModel.for_inference(model)\n",
                "print(\"âœ… íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. CoT ê¸°ë°˜ ìƒì„± í•¨ìˆ˜"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_post_v2(\n",
                "    board_name: str,\n",
                "    keywords: list,\n",
                "    factor_name: str = \"ì¼ë°˜\",\n",
                "    temperature: float = 0.7,\n",
                "    max_new_tokens: int = 1024,\n",
                ") -> dict:\n",
                "    \"\"\"\n",
                "    CoT ê¸°ë°˜ ì¸ê¸°ê¸€ ìƒì„±.\n",
                "    ëª¨ë¸ì´ Thinking â†’ Responseë¥¼ ìˆœì°¨ ìƒì„±í•˜ê³ , ê°ê° ë¶„ë¦¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
                "    \"\"\"\n",
                "    # íŒ©í„° í†µê³„ ì •ë³´\n",
                "    stats = factor_stats.get(factor_name, {})\n",
                "    coverage = stats.get(\"coverage\", 0)\n",
                "    factor_info = f\"{factor_name} (ì»¤ë²„ë¦¬ì§€: {coverage:.0%})\" if factor_name != \"ì¼ë°˜\" else \"ì¼ë°˜\"\n",
                "\n",
                "    instruction = f\"ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ì¸ê¸°ìš”ì¸ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ {board_name}ì˜ ì¸ê¸°ê¸€ ìŠ¤íƒ€ì¼ë¡œ ê¸€ì„ ì‘ì„±í•´ì¤˜.\"\n",
                "    input_text = f\"í‚¤ì›Œë“œ: {', '.join(keywords)}\\nì¸ê¸°ìš”ì¸: {factor_info}\"\n",
                "\n",
                "    BOS = tokenizer.bos_token\n",
                "    prompt = (\n",
                "        f\"{BOS}Below is an instruction that describes a task. \"\n",
                "        f\"Write a response that appropriately completes the request.\\n\\n\"\n",
                "        f\"### Instruction:\\n{instruction}\\n\\n\"\n",
                "        f\"### Input:\\n{input_text}\\n\\n\"\n",
                "        f\"### Thinking:\\n\"\n",
                "    )\n",
                "\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "\n",
                "    # Llama-3 EOS í† í° ì„¤ì •\n",
                "    eos_ids = [tokenizer.eos_token_id]\n",
                "    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
                "    if eot_id != tokenizer.unk_token_id:\n",
                "        eos_ids.append(eot_id)\n",
                "\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=max_new_tokens,\n",
                "        temperature=temperature,\n",
                "        do_sample=True,\n",
                "        pad_token_id=tokenizer.eos_token_id,\n",
                "        eos_token_id=eos_ids,\n",
                "        repetition_penalty=1.2,\n",
                "        no_repeat_ngram_size=3,\n",
                "    )\n",
                "\n",
                "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "    # â”€â”€ Thinking / Response ë¶„ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "    thinking = \"\"\n",
                "    response = \"\"\n",
                "\n",
                "    if \"### Thinking:\" in generated_text and \"### Response:\" in generated_text:\n",
                "        parts = generated_text.split(\"### Response:\")\n",
                "        response = parts[-1].strip()\n",
                "\n",
                "        thinking_part = parts[0]\n",
                "        if \"### Thinking:\" in thinking_part:\n",
                "            thinking = thinking_part.split(\"### Thinking:\")[-1].strip()\n",
                "    elif \"### Response:\" in generated_text:\n",
                "        response = generated_text.split(\"### Response:\")[-1].strip()\n",
                "    else:\n",
                "        response = generated_text.strip()\n",
                "\n",
                "    # â”€â”€ ì œëª©/ë³¸ë¬¸ ë¶„ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "    lines = response.strip().split(\"\\n\")\n",
                "    if len(lines) > 1:\n",
                "        title = lines[0].strip()\n",
                "        content = \"\\n\".join(lines[1:]).strip()\n",
                "    else:\n",
                "        title = response[:50].strip() + \"...\" if len(response) > 50 else response.strip()\n",
                "        content = response.strip()\n",
                "\n",
                "    return {\n",
                "        \"board_name\": board_name,\n",
                "        \"keywords\": \", \".join(keywords),\n",
                "        \"factor\": factor_name,\n",
                "        \"thinking\": thinking,\n",
                "        \"generated_title\": title,\n",
                "        \"generated_content\": content,\n",
                "    }\n",
                "\n",
                "\n",
                "print(\"âœ… CoT ê¸°ë°˜ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. ê²Œì‹œíŒë³„ + íŒ©í„°ë³„ ìƒì„± ì‹¤í–‰"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datetime import datetime\n",
                "\n",
                "generated_posts = []\n",
                "\n",
                "# ê° ê²Œì‹œíŒë³„ë¡œ, ìƒìœ„ 2ê°œ íŒ©í„°ì— ëŒ€í•´ ìƒì„±\n",
                "top_factors = factor_names[:3] if len(factor_names) >= 3 else factor_names\n",
                "\n",
                "for board, keywords in trend_keywords.items():\n",
                "    if not keywords:\n",
                "        continue\n",
                "\n",
                "    for factor in top_factors:\n",
                "        print(f\"\\nğŸ”§ ìƒì„± ì¤‘: [{board}] íŒ©í„°={factor}\")\n",
                "        print(f\"   í‚¤ì›Œë“œ: {', '.join(keywords[:5])}...\")\n",
                "\n",
                "        try:\n",
                "            result = generate_post_v2(\n",
                "                board_name=board,\n",
                "                keywords=keywords,\n",
                "                factor_name=factor,\n",
                "                temperature=0.7,\n",
                "                max_new_tokens=1024,\n",
                "            )\n",
                "            result[\"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
                "            generated_posts.append(result)\n",
                "\n",
                "            print(f\"   âœ… ìƒì„± ì™„ë£Œ\")\n",
                "            print(f\"   ğŸ’­ Thinking: {result['thinking'][:100]}...\")\n",
                "            print(f\"   ğŸ“ ì œëª©: {result['generated_title']}\")\n",
                "            print(f\"   ğŸ“„ ë‚´ìš©: {result['generated_content'][:150]}...\")\n",
                "\n",
                "        except Exception as exc:\n",
                "            print(f\"   âš ï¸ ìƒì„± ì‹¤íŒ¨: {exc}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. ìƒì„± ê²°ê³¼ ì €ì¥ ë° ë¹„êµ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if generated_posts:\n",
                "    result_df = pd.DataFrame(generated_posts)\n",
                "    output_csv = OUTPUT_DIR / \"generated_posts_v2.csv\"\n",
                "    result_df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
                "    print(f\"\\nâœ… ìƒì„± ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_csv}\")\n",
                "\n",
                "    print(\"\\n\" + \"=\" * 70)\n",
                "    print(\"ìƒì„± ê²°ê³¼ ìš”ì•½\".center(70))\n",
                "    print(\"=\" * 70)\n",
                "    display(result_df[[\"board_name\", \"factor\", \"generated_title\"]].head(20))\n",
                "else:\n",
                "    print(\"âš ï¸ ìƒì„±ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ ìƒì„¸ ì¶œë ¥: Thinking + Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"CoT ìƒì„¸ ì¶œë ¥ (Thinking â†’ Response)\".center(70))\n",
                "print(\"=\" * 70)\n",
                "\n",
                "for i, post in enumerate(generated_posts[:5], 1):\n",
                "    print(f\"\\n[{i}] [{post['board_name']}] íŒ©í„°: {post['factor']}\")\n",
                "    print(f\"    í‚¤ì›Œë“œ: {post['keywords']}\")\n",
                "    print(f\"\\n    ğŸ’­ Thinking:\")\n",
                "    print(f\"    {post['thinking'][:200]}\")\n",
                "    print(f\"\\n    ğŸ“ Response:\")\n",
                "    print(f\"    ì œëª©: {post['generated_title']}\")\n",
                "    print(f\"    ë‚´ìš©: {post['generated_content'][:300]}\")\n",
                "    print(\"-\" * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. ì¶”ê°€ íŒ\n",
                "- ìƒì„± ê²°ê³¼ë¥¼ ìˆ˜ì‘ì—…ìœ¼ë¡œ ê²€í† í•˜ì—¬ í’ˆì§ˆ í™•ì¸ í›„ ë°°í¬í•˜ì„¸ìš”.\n",
                "- `temperature`, `factor_name`, í‚¤ì›Œë“œ ì¡°í•©ì„ ì¡°ì •í•˜ì—¬ ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "- íŠ¹ì • íŒ©í„°ë¥¼ ì§€ì •í•˜ë©´ í•´ë‹¹ ì¸ê¸°ìœ í˜•ì— ë§ëŠ” ê¸€ì´ ìƒì„±ë©ë‹ˆë‹¤.\n",
                "- CoTì˜ Thinking ë¶€ë¶„ì´ ìƒì„± ì „ëµì„ ë³´ì—¬ì£¼ë¯€ë¡œ, ëª¨ë¸ì˜ ì¶”ë¡  ê³¼ì •ì„ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "accelerator": "GPU",
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}